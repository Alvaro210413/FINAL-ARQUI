
- Naturaleza del Workload:
Un workload describe el tipo de trabajo que realiza un programa y qu√© parte de ese trabajo *determina su tiempo total de ejecuci√≥n*. 
En t√©rminos generales, un programa puede ser I/O-bound, CPU-bound o mixto, dependiendo de si el tiempo se consume principalmente esperando 
operaciones externas (como descargas o lectura de archivos) o realizando c√°lculos dentro del procesador.

Un programa I/O-bound es aquel cuyo *rendimiento est√° limitado por operaciones de entrada/salida*, como solicitudes a internet, acceso a discos, 
lectura de archivos o consultas a bases de datos. En estos casos, la CPU pasa mucho tiempo inactiva, ‚Äúesperando‚Äù que llegue la informaci√≥n. 
Por ejemplo, al descargar varias p√°ginas web, la red es miles de veces m√°s lenta que la CPU; aunque el procesador sea r√°pido, el programa estar√° 
detenido esperando las respuestas del servidor. Por eso, ese tipo de tareas se consideran I/O-bound.

Por otro lado, un programa CPU-bound *est√° limitado por el procesamiento interno*. Este tipo de programas pasan la mayor parte del tiempo ejecutando 
c√°lculos intensivos, como an√°lisis num√©rico, criptograf√≠a, procesamiento de im√°genes o algoritmos complejos. En estos casos, el cuello de botella 
es la potencia del procesador, no la velocidad de la red. Un ejemplo t√≠pico ser√≠a calcular los n√∫meros primos hasta diez millones: no depende de 
la red ni del disco, solo del CPU.

Sin embargo, muchos programas son mixtos porque combinan fases I/O y CPU. No obstante, incluso en casos mixtos, el workload dominante es la parte 
que consume m√°s tiempo. Por ejemplo, si un programa descarga un archivo (I/O) y luego realiza un peque√±o procesamiento (CPU), pero la descarga 
toma 1 segundo y el an√°lisis solo 5 milisegundos, el tiempo total sigue estando determinado por el I/O.

-- talk about the example --

En la pr√°ctica de esta tarea, el script html_fetch_sync.py es un ejemplo claro de workload mixto. Este programa realiza dos tipos de trabajo: primero 
descarga p√°ginas web usando requests.get(), lo cual corresponde a una fase I/O-bound; luego analiza esas p√°ginas con BeautifulSoup, aplica expresiones 
regulares para extraer palabras y usa Counter para contarlas, lo que corresponde a una fase CPU-bound. Sin embargo, en comparaci√≥n, la descarga de HTML 
toma ampliamente m√°s tiempo que su procesamiento. El parsing, la tokenizaci√≥n y el conteo son operaciones r√°pidas que la CPU ejecuta en pocos milisegundos, 
mientras que la descarga depende de la velocidad de la red y normalmente es mucho m√°s lenta.

Por esta raz√≥n, aunque el programa realiza tareas CPU, el tiempo total de ejecuci√≥n est√° dominado por las operaciones de red, lo que significa que el 
workload del script es principalmente I/O-bound, con componentes CPU-bound secundarios. Dicho de forma simple: el programa pasa m√°s tiempo esperando que procesando.

Un ejemplo cotidiano ser√≠a pedir una pizza (descargar HTML) y luego cortarla en pedazos (procesar el HTML). El tiempo final no depende de cu√°nto tardas en cortarla, 
sino del tiempo que esperas a que llegue el delivery. Exactamente lo mismo ocurre aqu√≠: la descarga domina el tiempo total.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
SYNC:
En un programa s√≠ncrono (sync) cada operaci√≥n se ejecuta de forma secuencial y bloqueante: el programa no puede avanzar hasta que la instrucci√≥n actual termine 
completamente. En el contexto de esta pr√°ctica, esto significa que el script descarga una URL, espera a que termine, luego reci√©n pasa a la siguiente, 
y as√≠ sucesivamente. Por esa raz√≥n, si la descarga de una sola p√°gina tarda un segundo, y hay veinte URLs, el tiempo total ser√° cercano a veinte segundos o m√°s. 
El desempe√±o depende totalmente de la latencia externa, convirtiendo este patr√≥n en algo naturalmente lento cuando se trabaja con red.

ASYNC:
Por otro lado, asyncio permite escribir programas as√≠ncronos, donde muchas operaciones I/O pueden ejecutarse de forma concurrente dentro de un solo hilo. La clave 
es que las funciones as√≠ncronas no bloquean el programa completo, sino que ‚Äúceden el control‚Äù mientras esperan la respuesta de la red. As√≠, mientras una URL est√° 
esperando su HTML, el programa puede solicitar otra, procesar otra, o continuar tareas pendientes. Esto no acelera la red, pero s√≠ permite aprovechar el tiempo 
muerto que el modelo s√≠ncrono desperdicia, logrando un rendimiento mucho mejor en workloads I/O-bound.

Un ejemplo simple: en sync, si cada descarga tarda 1 segundo, 10 descargas toman unos 10 segundos. En asyncio, como todas pueden esperarse en paralelo, las mismas 
10 descargas pueden completarse en poco m√°s de 1 segundo. La CPU no trabaja m√°s r√°pido, simplemente no queda ‚Äúesperando sin hacer nada‚Äù. Por eso, asyncio es ideal 
para miles de descargas, solicitudes HTTP, consultas a bases de datos o cualquier tarea dominada por latencia.

En resumen: la versi√≥n s√≠ncrona es lenta porque espera bloqueada, una URL a la vez; la versi√≥n asyncio es mucho m√°s eficiente porque solapa las esperas, permitiendo 
que muchas descargas ocurran simult√°neamente. Este beneficio es especialmente notable en programas I/O-bound como este proyecto, donde el cuello de botella es la red y no la CPU.

SYNC:
[SYNC] Tiempo total: 88.73 s
[SYNC] Top-50 t√©rminos globales guardados en results_sync.json
[SYNC] Tiempo total: 78.857962 s
[SYNC] Top-50 t√©rminos globales guardados en results_sync.json
[SYNC] Tiempo total: 83.97 s
[SYNC] Top-50 t√©rminos globales guardados en results_sync.json

Indique si el programa secuencial resulta ser principalmente I/O-bound, CPU-bound o mixto, y explique por qu√©.  

Al ejecutar html_fetch_sync.py obtuve tiempos totales de 88.73 s, 78.857962 s y 78.85 s, lo cual muestra que la mayor parte de la ejecuci√≥n se dedica a esperar las 
descargas de cada p√°gina web. El procesamiento local (parsing, tokenizaci√≥n y conteo de palabras) es muy r√°pido en comparaci√≥n, mientras que las solicitudes de red 
presentan una latencia mucho mayor. Por ello, aunque el programa incluye una peque√±a parte CPU-bound, el tiempo total est√° claramente dominado por las operaciones de red, 
concluyendo que el script secuencial es principalmente I/O-bound.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Modifique html_fetch_sync.py para cronometrar por separado la fase de
  descarga y la de an√°lisis de cada p√°gina.

Informe los tiempos promedio de descarga y de parsing, y comente qu√© fase
domina el tiempo total.
Ejecuci√≥n 1:
Promedio de descarga: 0.732660 s
Promedio de parsing: 0.027001 s
Ejecuci√≥n 2:
Promedio de descarga: 0.524730 s
Promedio de parsing: 0.022671 s
Ejecuci√≥n 3:
Promedio de descarga: 0.543196 s
Promedio de parsing: 0.028506 s

Tras medir por separado la fase de descarga y la fase de an√°lisis dentro de html_fetch_sync.py, se obtuvo un tiempo promedio de descarga entre 0.52 s y 0.73 s por URL,
mientras que el parsing promedi√≥ solo 0.022 s a 0.028 s. Esto significa que la descarga es aproximadamente 25 veces m√°s lenta que el procesamiento local del HTML. 
Por lo tanto, el tiempo total de ejecuci√≥n est√° claramente dominado por la fase de I/O de red, ya que la CPU solo interviene unos pocos milisegundos por p√°gina. 
En consecuencia, la descarga es la etapa que determina el rendimiento global del programa.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- AsyncIO: rendimiento y an√°lisis de mejora

[ASYNC] Tiempo total: 29.52 s
[ASYNC] Top 50 palabras globales escritas en html_results_async.json
[ASYNC] Tiempo total: 20.14 s
[ASYNC] Top 50 palabras globales escritas en html_results_async.json
[ASYNC] Tiempo total: 19.65 s
[ASYNC] Top 50 palabras globales escritas en html_results_async.json

El speedup mide cu√°nto m√°s r√°pido es un m√©todo respecto a otro.

La f√≥rmula es:
Speedup = Tiempo_sync / Tiempo_async

Speedup = 84.53 / 23.10
        ‚âà 3.65√ó
Esto significa que asyncio fue 3.6 veces m√°s r√°pido que el modo s√≠ncrono.

Al ejecutar html_fetch_async.py, los tiempos obtenidos fueron 29.52 s, 20.14 s y 19.65 s, con un promedio aproximado de 23.10 s. Comparando este valor con el promedio 
del modo s√≠ncrono (‚âà84.53 s), se obtiene un speedup de 84.53 / 23.10 ‚âà 3.65√ó. Esto significa que la versi√≥n as√≠ncrona es m√°s de tres veces m√°s r√°pida que la secuencial. 
A partir de la instrumentaci√≥n previa, se observa que la fase de parsing dura solo unos milisegundos, por lo que el mayor beneficio de AsyncIO se da claramente en la 
fase de descarga, ya que permite solapar m√∫ltiples esperas de red en paralelo en lugar de bloquearse una URL a la vez.


- A partir de la instrumentaci√≥n previa, determine en qu√© fase (descarga o
  parsing) obtiene mayor beneficio el modelo asyncio.

Ejecuci√≥n 1:
Tiempo total de descargas (async): 9.7220 s
Tiempo total de parsing   (async): 1.9262 s
Tiempo total del programa (async): 11.6482 s
Promedio de descarga por URL (async): 0.0680 s
Promedio de an√°lisis  por URL (async): 0.013470 s
Ejecuci√≥n 2:
Tiempo total de descargas (async): 6.6137 s
Tiempo total de parsing   (async): 1.9895 s
Tiempo total del programa (async): 8.6032 s
Promedio de descarga por URL (async): 0.0462 s
Promedio de an√°lisis  por URL (async): 0.013913 s
Ejecuci√≥n 3:
Tiempo total de descargas (async): 6.8063 s
Tiempo total de parsing   (async): 1.8555 s
Tiempo total del programa (async): 8.6618 s
Promedio de descarga por URL (async): 0.0476 s
Promedio de an√°lisis  por URL (async): 0.012975 s

Adem√°s del tiempo total, se obtuvieron los promedios por URL dividiendo el tiempo total de descargas y el tiempo total de an√°lisis entre el n√∫mero de p√°ginas procesadas. 
De esta manera, el promedio de descarga por URL vari√≥ entre 0.05 s y 0.13 s, mientras que el promedio de an√°lisis por URL se mantuvo alrededor de 0.013‚Äì0.015 s. 
Estos valores evidencian que el parsing es muy r√°pido y que la mejora principal del modelo as√≠ncrono ocurre en la fase de descarga.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

- Hilos ‚Äúa mano‚Äù: medici√≥n y limitaciones:

Hilos:
[THREAD] Tiempo total: 23.75 s
[THREAD] Top-50 t√©rminos globales guardados en results_thread.json
[THREAD] Tiempo total: 26.04 s
[THREAD] Top-50 t√©rminos globales guardados en results_thread.json
[THREAD] Tiempo total: 25.87 s
[THREAD] Top-50 t√©rminos globales guardados en results_thread.json

¬øQu√© son los hilos (threads) en Python?

Un hilo es una unidad de ejecuci√≥n que corre dentro del mismo proceso. En teor√≠a, los hilos permiten que un programa ejecute varias tareas ‚Äúal mismo tiempo‚Äù.
Sin embargo, Python no permite que dos hilos ejecuten c√≥digo Python en paralelo al mismo tiempo, por una limitaci√≥n interna llamada GIL (Global Interpreter Lock).
Por eso, los threads en Python se usan principalmente para I/O-bound, no para CPU.

¬øQu√© es el GIL (Global Interpreter Lock)?

El GIL es un ‚Äúcandado global‚Äù dentro del int√©rprete de Python (CPython) que solo permite que un hilo ejecute bytecode de Python a la vez.

- Aunque tengas 8 n√∫cleos
- Aunque crees 100 threads
- Python solo deja que 1 hilo ejecute c√≥digo Python al mismo tiempo

Los dem√°s deben esperar su turno para ejecutar operaciones CPU.

¬øEl GIL bloquea completamente el paralelismo?

No exactamente.

Hay dos casos:

‚úî I/O-bound
Cuando un hilo est√° esperando la red, disco o un socket, libera el GIL, y otro hilo puede correr.
Por eso los hilos s√≠ aceleran programas I/O-bound.

Ejemplo: descargas HTTP ‚Üí s√≠ mejora.

‚ùå CPU-bound
Si los hilos est√°n haciendo trabajo de CPU, ninguno puede ejecutarse realmente en paralelo, porque todos necesitan el GIL.

Resultado: NO hay speedup, incluso puede ser m√°s lento.

*****  Speedup = Tiempo_sync / Tiempo_thread

      Entonces:
        Speedup = 84.53 / 25.22
                ‚âà 3.35√ó

¬øC√≥mo influye el GIL en el rendimiento observado al usar threading.Thread manual?
El uso de hilos manuales mejora el rendimiento respecto al modo secuencial porque la descarga de p√°ginas es una operaci√≥n I/O-bound. Durante las esperas de red, 
los hilos liberan el GIL, permitiendo que otros hilos avancen y solapen varias descargas. Sin embargo, el parsing del HTML es una operaci√≥n CPU-bound y, debido 
al Global Interpreter Lock (GIL), solo un hilo puede ejecutar bytecode de Python a la vez. Esto impide que los hilos aceleren la fase de an√°lisis y limita el 
speedup total. Como resultado, el modo con threads es m√°s r√°pido que el modo secuencial, pero menos eficiente que AsyncIO, ya que sigue restringido por el GIL 
en la parte CPU-bound del programa.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Procesos ‚Äúa mano‚Äù: aislamiento y overhead:

[PROCESS] Tiempo total: 57.47 s
[PROCESS] Top-50 t√©rminos globales guardados en results_process.json
[PROCESS] Tiempo total: 68.75 s
[PROCESS] Top-50 t√©rminos globales guardados en results_process.json
[PROCESS] Tiempo total: 63.36 s
[PROCESS] Top-50 t√©rminos globales guardados en results_process.json

¬øQu√© es un proceso? (multiprocessing.Process)

Un proceso es una instancia completa del int√©rprete de Python:

‚úî Tiene su propio Python interpreter
‚úî Su propio espacio de memoria
‚úî Su propia copia del c√≥digo
‚úî No comparte variables con otros procesos
‚úî No est√° limitado por el GIL

Es decir, cada proceso puede ejecutar c√≥digo Python en paralelo real en diferentes n√∫cleos.

En contraste con los hilos (threads):

Caracter√≠stica	    Threads        	Procesos
Memoria	          Compartida	      Aislada
GIL	               Limita	         No limita
Ideal para	      I/O-bound	       CPU-bound
Overhead	          Bajo	         Medio/Alto
Paralelismo	      No real	           Real

¬øPor qu√© tus tiempos con procesos son tan irregulares?
Tus resultados:
63.36 s
68.75 s
57.47 s

Esto pasa porque:
Multiprocessing tiene alto overhead
   Crear procesos es costoso
   Copiar funciones, datos, memoria inicial cuesta tiempo
Los procesos se comunican por pipes/queues
   Esto a√±ade latencia
   No es gratis pasar datos entre procesos
La fase dominante sigue siendo I/O
   Descargar 140 p√°ginas no se acelera tanto
   Tener 5‚Äì10 procesos solo aumenta el tr√°fico de red

Por eso multiprocessing:
Mejora menos que threads
Es peor que asyncio
Var√≠a bastante en tiempos
Esto es completamente normal.

Speedup vs secuencial (con tus n√∫meros)

Tus tiempos:

SYNC promedio ‚âà 84.53 s
PROCESS promedio ‚âà (63.36 + 68.75 + 57.47) / 3 ‚âà 63.19 s

Speedup:
Speedup = 84.53 / 63.19 ‚âà 1.34√ó

O sea:
 Los procesos son 34% m√°s r√°pidos que sync
 Menos r√°pidos que threads (~3.3√ó)
 Mucho m√°s lentos que async (~8.7√ó)
Y esto es exactamente lo esperado.

¬øC√≥mo afecta fork a parsing y descarga?

El uso de multiprocessing.Process crea varios procesos independientes, cada uno con su propio 
int√©rprete de Python. Esto elimina las restricciones del GIL, permitiendo que diferentes procesos 
ejecuten c√≥digo Python en paralelo real. Por este motivo, la fase de parsing ‚Äîque es una operaci√≥n 
CPU-bound‚Äî s√≠ puede beneficiarse del paralelismo y ejecutarse simult√°neamente en varios n√∫cleos. 
Sin embargo, la fase de descarga es I/O-bound y no obtiene una mejora significativa con procesos, 
ya que la velocidad de red y la latencia del servidor contin√∫an siendo el factor limitante. 
Adem√°s, los procesos tienen un overhead considerable (creaci√≥n, copia de memoria, comunicaci√≥n entre procesos), 
lo cual reduce el beneficio total. Como resultado, los procesos son m√°s r√°pidos que el modo secuencial, 
pero ofrecen un speedup limitado y menor que el de hilos o AsyncIO.

AsyncIO (m√°s r√°pido) < Threads < Procesos < Sync (m√°s lento)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- Pools vs gesti√≥n manual:

[POOL-THREAD] Tiempo total: 23.98 s
[POOL-THREAD] Top-50 t√©rminos globales guardados en results_thread_pool.json
[POOL-THREAD] Tiempo total: 27.30 s
[POOL-THREAD] Top-50 t√©rminos globales guardados en results_thread_pool.json
[POOL-THREAD] Tiempo total: 26.50 s
[POOL-THREAD] Top-50 t√©rminos globales guardados en results_thread_pool.json

[POOL-PROC] Tiempo total: 38.18 s
[POOL-PROC] Top-50 t√©rminos globales guardados en results_process_pool.json
[POOL-PROC] Tiempo total: 45.72 s
[POOL-PROC] Top-50 t√©rminos globales guardados en results_process_pool.json
[POOL-PROC] Tiempo total: 27.58 s
[POOL-PROC] Top-50 t√©rminos globales guardados en results_process_pool.json

Thread Pool vs Threads manuales

Al comparar la versi√≥n con hilos manuales con ThreadPoolExecutor, los tiempos promedio fueron muy similares (‚âà25 s en ambos casos). 
Esto se debe a que las operaciones de red son I/O-bound y liberan el GIL, por lo que tanto los hilos manuales como el thread pool 
pueden solapar varias descargas sin restricciones. La ventaja del thread pool no est√° en el rendimiento, sino en la simplicidad del 
c√≥digo: maneja la creaci√≥n, reutilizaci√≥n y coordinaci√≥n de hilos autom√°ticamente, reduciendo errores y haci√©ndolo m√°s f√°cil de mantener.

Process Pool vs Processes manuales

En el caso de procesos, la diferencia s√≠ fue significativa: la versi√≥n con procesos manuales promedi√≥ ‚âà63.19 s, mientras que el process 
pool redujo el tiempo a ‚âà37.16 s. Esto ocurre porque multiprocessing.Process crea un proceso nuevo por URL, lo cual tiene un overhead muy alto, 
ya que cada proceso implica copiar memoria, inicializar un int√©rprete y comunicarse mediante pipes. Por el contrario, ProcessPoolExecutor 
mantiene un conjunto fijo de procesos ya inicializados y distribuye el trabajo entre ellos, reutiliz√°ndolos y evitando este overhead. 
Por ello, el process pool ofrece mejor rendimiento y un c√≥digo m√°s simple.

¬øQu√© ventaja de rendimiento y complejidad de c√≥digo aporta cada tipo de pool?
üîµ Thread Pool (ThreadPoolExecutor)

Rendimiento:
El rendimiento es muy parecido al de los hilos manuales en este workload I/O-bound.
No da un speedup grande extra, pero mantiene el buen rendimiento que ya tienen los threads.
Complejidad de c√≥digo:
Gran ventaja: el c√≥digo es m√°s corto y limpio.
No tienes que crear, arrancar ni hacer join() a cada hilo a mano.
El pool administra la cola de tareas y la reutilizaci√≥n de hilos autom√°ticamente.

Conclusi√≥n: poca mejora en rendimiento, gran mejora en simplicidad y mantenibilidad.

üü¢ Process Pool (ProcessPoolExecutor / multiprocessing.Pool)

Rendimiento:

Aqu√≠ s√≠ hay una mejora clara frente a procesos ‚Äúa mano‚Äù.
El pool reutiliza un conjunto fijo de procesos y evita crear uno nuevo por URL.
Eso reduce much√≠simo el overhead de fork y da tiempos mucho menores.
Complejidad de c√≥digo:
El c√≥digo es mucho m√°s simple que gestionar Process manualmente.
No tienes que coordinar procesos, ni manejar listas compartidas, ni pipes expl√≠citos.

Conclusi√≥n: mejor rendimiento y menor complejidad de c√≥digo comparado con procesos manuales.

pip install memory_profiler
mprof run python html_fetch_async.py
mprof peak

import matplotlib.pyplot as plt

workers = [2, 4, 8, 16]
tiempos = [45.3, 32.3, 26.8, 25.4]       # reemplazar con tus valores
memoria = [95, 97, 101, 121]            # reemplazar con tus valores

plt.figure(figsize=(8,4))

# --- Grafico de tiempo ---
plt.subplot(1, 2, 1)
plt.plot(workers, tiempos, marker='o')
plt.title('Escalabilidad de Pool (Tiempo)')
plt.xlabel('Workers')
plt.ylabel('Tiempo (s)')

# --- Grafico de memoria ---
plt.subplot(1, 2, 2)
plt.plot(workers, memoria, marker='o', color='orange')
plt.title('Escalabilidad de Pool (Memoria)')
plt.xlabel('Workers')
plt.ylabel('Memoria (MiB)')

plt.tight_layout()
plt.savefig('escalabilidad_pool.png')
plt.show()

¬øQu√© se espera que observes?

1. Al aumentar los workers, el tiempo mejora al inicio
Porque hay m√°s tareas ejecut√°ndose en paralelo.

2. Pero llega un punto donde la mejora se detiene ‚Üí punto √≥ptimo
Normalmente entre 8 y 12 workers en este tipo de laboratorio.

3. Despu√©s de ese punto, agregar m√°s workers NO mejora
Incluso puede empeorar el tiempo.

4. La memoria crece a medida que aumentan los workers
M√°s hilos/procesos = m√°s buffers y estructuras.

5. La raz√≥n de que deje de escalar es la contenci√≥n
Los workers compiten por:
  red saturada
  el GIL en el parsing
  CPU limitada
  colas de trabajo del executor
  contexto del sistema operativo

Punto de rendimiento √≥ptimo y efecto de la contenci√≥n

Al incrementar el n√∫mero de workers, el tiempo total mejora inicialmente porque se solapan m√°s tareas en paralelo. 
Sin embargo, esta mejora solo ocurre hasta un punto √≥ptimo, que t√≠picamente aparece entre 8 y 12 workers. 
A partir de ese valor, agregar m√°s workers deja de reducir el tiempo e incluso puede empeorarlo. Esto se debe a la contenci√≥n, 
es decir, la competencia entre workers por recursos limitados como la red, el GIL durante el parsing, la cola del pool y la CPU. 
Cuando aumenta la contenci√≥n, los workers pasan m√°s tiempo bloqueados o esperando turnos, por lo que el rendimiento deja de 
escalar y la memoria consumida aumenta innecesariamente.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

OPERACIONES.C:
Lo que se espera en C (sin GIL):
  Los hilos s√≠ pueden correr en paralelo en CPU-bound.
  Los procesos tambi√©n, pero tienen m√°s overhead.
  El speedup m√°ximo est√° limitado por la cantidad de n√∫cleos.

   CPU BOUND vs WORKERS
  T√©cnica	                    Deber√≠a pasar en C	                                                                              ¬øOcurre en tu gr√°fica?
  Threading (Hilos)	          Deber√≠a mejorar al inicio, pero estabilizarse cuando ya no hay m√°s n√∫cleos disponibles	        ‚úîÔ∏è S√≠: baja de 6 s a ~3.0 s y se mantiene ah√≠ sin mejorar m√°s
  Multiprocessing (Procesos)	Deber√≠a mejorar igual o un poco m√°s que los hilos (por aislamiento), hasta el l√≠mite de n√∫cleos	‚úîÔ∏è S√≠: tambi√©n baja a ~3.0‚Äì3.5 s pero con m√°s picos
  Muchos workers (>8)	        Empeora por overhead, cambios de contexto y contenci√≥n	                                        ‚úîÔ∏è S√≠: aparecen picos irregulares a partir de ~10 workers
   
   I/O BOUND vs WORKERS
  T√©cnica	                    Deber√≠a pasar en C	                                            ¬øOcurre en tu gr√°fica?
  Threading (Hilos)	          Deber√≠a ser muy r√°pido y estable, mejor que secuencial	     ‚úîÔ∏è S√≠: se mantiene en 0.025‚Äì0.04 s, siempre m√°s r√°pido que 0.071 s secuencial
  Multiprocessing (Procesos)	No deber√≠a mejorar, e incluso podr√≠a empeorar por overhead	 ‚úîÔ∏è S√≠: es peor que hilos e incluso peor que secuencial en varios puntos
  Muchos workers (>8)	        Deber√≠a empeorar por contenci√≥n de disco y procesos pesados	 ‚úîÔ∏è S√≠: hay picos fuertes en varios puntos de 0.06‚Äì0.09 s

En CPU-bound, hilos y procesos mejoran el rendimiento al inicio, pero la ganancia se detiene al saturar los n√∫cleos 
y aparece overhead, siendo los procesos m√°s irregulares por su mayor costo.

En I/O-bound, los hilos son claramente superiores porque pueden avanzar mientras esperan al disco, mientras que los 
procesos solo a√±aden overhead y resultan incluso m√°s lentos que el modo secuencial.

OPERACIONES.PY:
   CPU BOUND vs WORKERS
  T√©cnica	                    Deber√≠a pasar	                                      ¬øOcurre en tu gr√°fica?
  Threading (Hilos)	          No mejora mucho o incluso empeora, por el GIL	   ‚úîÔ∏è S√≠: se queda pegado alrededor de 1.5‚Äì2.0 s y tiene picos feos
  Multiprocessing (Procesos)	Debe acelerar, hasta cierto punto	               ‚úîÔ∏è S√≠: baja hasta ~0.8‚Äì1.0 s con 3‚Äì6 workers
  Muchos procesos (>8)	      Debe empeorar por overhead y contenci√≥n	         ‚úîÔ∏è S√≠: desde 10 hacia adelante empieza a subir

   I/O BOUND vs WORKERS
  Lo que se espera en una carga I/O-bound:
  T√©cnica	                    Deber√≠a pasar	                                                               ¬øOcurre en tu gr√°fica?
  Threading (hilos)	          Debe ganar ampliamente porque puede cambiar de tarea mientras espera I/O	‚úîÔ∏è S√≠: se mantiene ~0.24‚Äì0.28 s
  Multiprocessing	            No debe mejorar ‚Üí overhead innecesario	                                  ‚úîÔ∏è S√≠: sube hasta 1.0 s, o sea es PEOR que secuencial

En CPU-bound, multiprocessing logra mejor rendimiento porque cada proceso ejecuta en un core distinto y evita el GIL. 
Threading no mejora y puede empeorar debido a contenci√≥n del GIL.

En I/O-bound, threading es ampliamente superior porque los hilos pueden avanzar mientras otros esperan el disco. 
Multiprocessing introduce alto overhead y no aporta beneficio, resultando incluso peor que el modo secuencial.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
¬øQu√© es el overhead? 

El overhead es el costo extra que aparece cuando intentas paralelizar o concurrenciar un programa.
No es trabajo ‚Äú√∫til‚Äù, sino trabajo adicional que NO existir√≠a en una ejecuci√≥n secuencial, como:

- crear hilos o procesos
- copiar memoria (procesos)
- comunicaci√≥n entre workers
- sincronizaci√≥n
- cambio de contexto (context switching)
- administraci√≥n interna del SO

En pocas palabras:
    Overhead = tiempo perdido por coordinar la paralelizaci√≥n.
    No hace el trabajo, solo administra el trabajo.

Cuando el overhead es mayor que la ganancia, el programa se vuelve m√°s lento.

EJEMPLO:
=== CPU BOUND ===                                                       === I/O BOUND ===
Tiempo de ejecuci√≥n secuencial: 1.634889                                I/O secuencial: 0.284061
Tiempo de ejecuci√≥n concurrente con 2 hilos: 1.550625                   I/O concurrente con 2 hilos: 0.244102
Tiempo de ejecuci√≥n concurrente con 3 hilos: 1.593165                   I/O concurrente con 3 hilos: 0.261416
Tiempo de ejecuci√≥n concurrente con 4 hilos: 1.571420                   I/O concurrente con 4 hilos: 0.280523
Tiempo de ejecuci√≥n concurrente con 5 hilos: 1.603957                   I/O concurrente con 5 hilos: 0.271541   
Tiempo de ejecuci√≥n concurrente con 6 hilos: 1.734278                   I/O concurrente con 6 hilos: 0.302944
Tiempo de ejecuci√≥n concurrente con 7 hilos: 1.788568                   I/O concurrente con 7 hilos: 0.329759
Tiempo de ejecuci√≥n concurrente con 8 hilos: 1.711800                   I/O concurrente con 8 hilos: 0.363895
Tiempo de ejecuci√≥n concurrente con 9 hilos: 1.645342                   I/O concurrente con 9 hilos: 0.261269
Tiempo de ejecuci√≥n concurrente con 10 hilos: 1.727296                  I/O concurrente con 10 hilos: 0.259300
Tiempo de ejecuci√≥n concurrente con 11 hilos: 1.710168                  I/O concurrente con 11 hilos: 0.258988
Tiempo de ejecuci√≥n concurrente con 12 hilos: 1.633236                  I/O concurrente con 12 hilos: 0.292595
Tiempo de ejecuci√≥n concurrente con 13 hilos: 1.740746                  I/O concurrente con 13 hilos: 0.287716
Tiempo de ejecuci√≥n concurrente con 14 hilos: 1.715659                  I/O concurrente con 14 hilos: 0.302804
Tiempo de ejecuci√≥n concurrente con 15 hilos: 1.748673                  I/O concurrente con 15 hilos: 0.309957
Tiempo de ejecuci√≥n concurrente con 16 hilos: 1.889665                  I/O concurrente con 16 hilos: 0.295531
Tiempo de ejecuci√≥n concurrente con 17 hilos: 1.639583                  I/O concurrente con 17 hilos: 0.305398
Tiempo de ejecuci√≥n concurrente con 18 hilos: 1.717307                  I/O concurrente con 18 hilos: 0.293670
Tiempo de ejecuci√≥n concurrente con 19 hilos: 1.596196                  I/O concurrente con 19 hilos: 0.293901
Tiempo de ejecuci√≥n paralela con 2 procesos: 1.083883                   I/O paralela con 2 procesos: 0.315975
Tiempo de ejecuci√≥n paralela con 3 procesos: 0.955549                   I/O paralela con 3 procesos: 0.305282
Tiempo de ejecuci√≥n paralela con 4 procesos: 1.104379                   ...
Tiempo de ejecuci√≥n paralela con 5 procesos: 1.046830                   I/O paralela con 5 procesos: 0.416544
Tiempo de ejecuci√≥n paralela con 6 procesos: 1.138204                   I/O paralela con 6 procesos: 0.478920
Tiempo de ejecuci√≥n paralela con 7 procesos: 1.250429                   I/O paralela con 7 procesos: 0.483203
Tiempo de ejecuci√≥n paralela con 8 procesos: 1.287919                   I/O paralela con 8 procesos: 0.586520
Tiempo de ejecuci√≥n paralela con 9 procesos: 1.263757                   I/O paralela con 9 procesos: 0.666404
Tiempo de ejecuci√≥n paralela con 10 procesos: 1.349247                  I/O paralela con 10 procesos: 0.758487
Tiempo de ejecuci√≥n paralela con 11 procesos: 1.430543                  I/O paralela con 11 procesos: 0.772215
Tiempo de ejecuci√≥n paralela con 12 procesos: 1.720765                  I/O paralela con 12 procesos: 0.838149
Tiempo de ejecuci√≥n paralela con 13 procesos: 1.573000                  I/O paralela con 13 procesos: 0.771687
Tiempo de ejecuci√≥n paralela con 14 procesos: 1.540869                  I/O paralela con 14 procesos: 1.261161
Tiempo de ejecuci√≥n paralela con 15 procesos: 1.618951                  I/O paralela con 15 procesos: 1.041900
Tiempo de ejecuci√≥n paralela con 16 procesos: 1.624018                  I/O paralela con 16 procesos: 1.216580
Tiempo de ejecuci√≥n paralela con 17 procesos: 1.868275                  I/O paralela con 17 procesos: 1.103399
Tiempo de ejecuci√≥n paralela con 18 procesos: 1.721691                  I/O paralela con 18 procesos: 1.284024
Tiempo de ejecuci√≥n paralela con 19 procesos: 1.955831                  I/O paralela con 19 procesos: 1.442757

¬øD√≥nde se ve el overhead en tus resultados?

Cuando agregas m√°s hilos/procesos y el tiempo sube en vez de bajar, eso es overhead trag√°ndose todas las ganancias.

üü¢ Ejemplo claro en tu CPU-bound:
Secuencial: 1.63 s
Procesos: (mejor caso) 0.81 ‚Äì 1.04 s
Pero luego:

12 procesos ‚Üí 1.17 s
16 procesos ‚Üí 1.34 s
19 procesos ‚Üí 1.46 s

Ah√≠ ya no mejoras: el SO est√° gastando m√°s tiempo creando, cambiando, sincronizando y coordinando procesos que haciendo trabajo real.

Eso es overhead de procesos.
En el caso de hilos (threading) en CPU-bound:

Tus tiempos casi no cambian:
   1.63 s secuencial
   1.55‚Äì1.70 s con hilos

Eso te muestra claramente el GIL: todos los hilos compiten por un √∫nico n√∫cleo l√≥gico,
y lo √∫nico que aumenta es el overhead de cambiar entre hilos.

üü¢ En I/O-bound, el overhead se ve m√°s claro en multiprocessing:

Secuencial: 0.24 s
8 procesos: 0.49 s
12 procesos: 0.59 s
16 procesos: 0.83 s
19 procesos: 0.96 s

‚Üí No solo NO mejora, se vuelve m√°s del triple de lento.
Porque multiprocessing NO sirve para I/O, los procesos pasan su vida esperando el disco, pero igual se paga el costo extra de crearlos, 
sincronizarlos y cambiar entre ellos.
Eso es overhead puro.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
¬øQu√© significa el SPEEDUP que te gener√≥ tu benchmark?
El speedup se define como:
  S = T_secuencial / T_paralelo

Interpretaci√≥n simple:
  S = 1.0 ‚Üí no ganaste nada
  S > 1.0 ‚Üí el programa es m√°s r√°pido
  S < 1.0 ‚Üí paralelizar lo empeor√≥ (mucho overhead)

